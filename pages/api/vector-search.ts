import type { NextApiRequest, NextApiResponse } from 'next'
import { oneLine } from 'common-tags'
import { ApplicationError, UserError } from '@/lib/errors'
import { getConfig } from '@/lib/config'
import { getOpenAIClient, formatOpenAIError } from '@/lib/openai-client'
import { getServerSupabaseClient } from '@/lib/supabase-server'
import { writePlainTextHeaders, writeCitations, writeWithBackpressure, sendTextWithCitations } from '@/lib/http'
import { classifyIntentKorean } from '@/lib/intent'
import { matchSectionsForQuery, buildContextFromSections, buildKoreanLegalPrompt } from '@/lib/rag'

// ê³µìš© íƒ€ì…
// í•œê¸€ ì£¼ì„: ë¡œì»¬ íƒ€ì… ì œê±°(ê³µìš© ëª¨ë“ˆì˜ íƒ€ì… ì‚¬ìš©)

// ì‘ë‹µ í—¬í¼: ê³µí†µ í—¤ë” ì„¤ì •
// í—¤ë”/ì“°ê¸° ìœ í‹¸ì€ '@/lib/http' ì‚¬ìš©

// ì‘ë‹µ í—¬í¼: ì¸ìš© ì£¼ì„ ì „ì†¡
// ì¸ìš© ìœ í‹¸ì€ '@/lib/http' ì‚¬ìš©

// ì‘ë‹µ í—¬í¼: ë°±í”„ë ˆì…” ì•ˆì „ ì“°ê¸°
// ë°±í”„ë ˆì…” ìœ í‹¸ì€ '@/lib/http' ì‚¬ìš©

// ì‘ë‹µ í—¬í¼: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì‘ë‹µ ì „ì†¡
// í…ìŠ¤íŠ¸ ì „ì†¡ ìœ í‹¸ì€ '@/lib/http' ì‚¬ìš©

// ìœ í‹¸: ë¶ˆë¦¬ì–¸ íŒŒì‹± ("true"/true í—ˆìš©)
// ë¶ˆë¦¬ì–¸ íŒŒì‹± ìœ í‹¸ì€ '@/lib/http' ì‚¬ìš©

// ìœ í‹¸: ì„œë²„ìš© Supabase í´ë¼ì´ì–¸íŠ¸
// Supabase í´ë¼ì´ì–¸íŠ¸ëŠ” '@/lib/supabase-server' ì‚¬ìš©

// í•œê¸€ ì£¼ì„: ë¹Œë“œ ì‹œ í™˜ê²½ë³€ìˆ˜ ì˜ì¡´ì„ í”¼í•˜ê¸° ìœ„í•´ ëŸ°íƒ€ì„ì— ì´ˆê¸°í™”

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  try {
    // í•œê¸€ ì£¼ì„: ìš”ì²­ ë° í™˜ê²½ ì²´í¬
    console.log('ğŸš€ Vector search API í˜¸ì¶œ')
    if (req.method !== 'POST') {
      return res.status(405).json({ error: 'Method Not Allowed' })
    }

    // í•œê¸€ ì£¼ì„: êµ¬ì„± ë¡œë”©(ìœ íš¨ì„±ì€ getConfigì—ì„œ ë³´ì¥) ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    const { models } = getConfig()
    const openai = getOpenAIClient()

    const requestData = req.body
    if (!requestData) {
      throw new UserError('Missing request data')
    }

    const { prompt: query } = requestData
    // í•œê¸€ ì£¼ì„: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ìµœëŒ€ ê¸¸ì´(ê°œìˆ˜) ì˜µì…˜ ì²˜ë¦¬
    const history = Array.isArray(requestData?.history) ? (requestData.history as Array<{ role: string; content: string }>) : []
    const historyLimitEnv = Number(process.env.CHAT_HISTORY_LIMIT || process.env.NEXT_PUBLIC_CHAT_HISTORY_LIMIT || 3)
    const historyLimit = Number.isFinite(historyLimitEnv) && historyLimitEnv > 0 ? historyLimitEnv : 3
    const historyMax = Number.isFinite(Number(requestData?.historyLimit)) && Number(requestData?.historyLimit) > 0
      ? Math.min(Number(requestData.historyLimit), 10)
      : historyLimit
    const trimmedHistory = history.slice(-historyMax)
    // í•œê¸€ ì£¼ì„: ê¸°ë³¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³€ê²½ (í´ë¼ì´ì–¸íŠ¸ê°€ ëª…ì‹œì ìœ¼ë¡œ falseë¥¼ ë³´ë‚¼ ë•Œë§Œ ë¹„ìŠ¤íŠ¸ë¦¬ë°)
    const wantsStream = requestData?.stream === false ? false : true

    if (!query) {
      throw new UserError('Missing query in request data')
    }

    const supabaseClient = getServerSupabaseClient()

    // Moderate the content to comply with OpenAI T&C
    const sanitizedQuery = query.trim()
    const moderationResponse = await openai.moderations.create({
      model: models.moderation,
      input: sanitizedQuery,
    })

    // Vercel í™˜ê²½ì—ì„œ moderationResponse.resultsê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (
      !moderationResponse.results ||
      !Array.isArray(moderationResponse.results) ||
      moderationResponse.results.length === 0
    ) {
      throw new ApplicationError('Invalid moderation response from OpenAI')
    }

    const [results] = moderationResponse.results

    if (results.flagged) {
      throw new UserError('Flagged content', {
        flagged: true,
        categories: results.categories,
      })
    }

    // LLM ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜
    const { intent: classifiedIntent, confidence: classifiedConfidence } = await classifyIntentKorean(
      sanitizedQuery
    )

    // ì¸ì‚¬/ìŠ¤ëª°í†¡/ë¹„ë²•ë¥  ëŒ€ì‘ì€ RAG ìƒëµ í›„ ì¦‰ì‹œ ì‘ë‹µ
    if (classifiedIntent === 'greeting' || classifiedIntent === 'smalltalk') {
      const greetingAnswer = [
        'ì•ˆë…•í•˜ì„¸ìš”! ë²•ë¬´ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì–´ë–¤ ë²•ì  ë¬¸ì˜ë¥¼ ë„ì™€ë“œë¦´ê¹Œìš”?\n',
        '- ì˜ˆ: ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?\n',
        '- ì˜ˆ: ì§ì¥ì—ì„œ ë¶€ë‹¹í•œ ëŒ€ìš°ë¥¼ ë°›ì•˜ì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n',
        '- ì˜ˆ: ì„ëŒ€ì°¨ ê³„ì•½ ë§Œë£Œ í›„ ë³´ì¦ê¸ˆ ë°˜í™˜ ì ˆì°¨ê°€ ê¶ê¸ˆí•´ìš”.\n',
        '\ní•„ìš”í•˜ì‹œë‹¤ë©´ ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ë„ ë„ì™€ë“œë¦´ê²Œìš”. ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼)ê³¼ ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?'
      ].join('\n')
      sendTextWithCitations(res, greetingAnswer, [], sanitizedQuery)
      return
    }

    if (classifiedIntent === 'non_legal' || classifiedIntent === 'other') {
      // í•œê¸€ ì£¼ì„: ë¹„ë²•ë¥ /ê¸°íƒ€ ì£¼ì œì˜ ê²½ìš°ì—ë„ ê°„ë‹¨í•œ ì¼ë°˜ ì •ë³´ ì‘ë‹µì„ ì œê³µí•˜ê³  ìƒë‹´ ì—°ê²°ë¡œ ë¶€ë“œëŸ½ê²Œ ìœ ë„
      try {
        const nonLegalSystem = oneLine`
          ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” í•œêµ­ì–´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë²•ë¥  'ì™¸' ì£¼ì œì— ëŒ€í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¼ë°˜ ì •ë³´ ìˆ˜ì¤€ìœ¼ë¡œë§Œ ê°„ë‹¨íˆ(2~3ë¬¸ì¥) ë‹µí•©ë‹ˆë‹¤.
          ì „ë¬¸ì  ì¡°ì–¸ì´ë‚˜ í™•ì •ì  ë‹¨ì •ì€ í”¼í•˜ê³ , ì•ˆì „í•œ ë²”ìœ„ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”. ë§íˆ¬ëŠ” ì‚¬ìš©ì ì…ë ¥ì˜ í†¤ì„ ê°€ë³ê²Œ ë°˜ì˜í•˜ë˜ ê¸°ë³¸ì€ ì¡´ëŒ“ë§ì…ë‹ˆë‹¤.
          ì˜¤ì§ ê°„ê²°í•œ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.`
        const nlResp = await openai.responses.create({
          model: models.chat,
          instructions: nonLegalSystem,
          input: sanitizedQuery,
        })
        const shortAnswer = (nlResp as any).output_text?.trim() ?? ''
        const guidanceTail = [
          '',
          'í˜¹ì‹œ ë²•ì  ì´ìŠˆë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´, ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.',
          'ìƒë‹´ì„ ì›í•˜ì‹œë©´ ì„±í•¨ê³¼ ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼), ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?',
          'í˜„ì¬ ìƒí™©ì„ í•œë‘ ë¬¸ì¥ìœ¼ë¡œë§Œ ë§ë¶™ì—¬ ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ê²Œìš”.',
        ].join('\n')
        const finalAnswer = [shortAnswer, guidanceTail].join('\n')
        sendTextWithCitations(res, finalAnswer, [], sanitizedQuery)
      } catch (e) {
        // í•œê¸€ ì£¼ì„: ì‹¤íŒ¨ ì‹œì—ë„ ëŒ€í™”ë¥¼ ëŠì§€ ì•Šê³  ì•ˆë‚´ ë° CTA ì œê³µ
        const fallback = [
          'ê°„ë‹¨íˆ ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ê·¸ë˜ë„ ê±±ì • ë§ˆì„¸ìš”.',
          'ë²•ì  ì´ìŠˆë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.\n',
          'ìƒë‹´ì„ ì›í•˜ì‹œë©´ ì„±í•¨ê³¼ ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼), ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?',
        ].join('\n')
        sendTextWithCitations(res, fallback, [], sanitizedQuery)
      }
      return
    }

    // Create embedding from query
    const embeddingResponse = await openai.embeddings.create({
      model: models.embedding,
      input: sanitizedQuery.replaceAll('\n', ' '),
    })

    // Vercel í™˜ê²½ì—ì„œ embedding dataê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (!embeddingResponse.data || !Array.isArray(embeddingResponse.data) || embeddingResponse.data.length === 0) {
      throw new ApplicationError('Invalid embedding response from OpenAI')
    }

    const [{ embedding }] = embeddingResponse.data

    const pageSections = await matchSectionsForQuery(embedding)
    if (!pageSections || !Array.isArray(pageSections)) {
      throw new ApplicationError('No matching page sections found')
    }

    const { contextText, usedSections } = buildContextFromSections(pageSections as any)
    // í•œê¸€ ì£¼ì„: íˆìŠ¤í† ë¦¬ë¥¼ ì••ì¶•ëœ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    const historyText = trimmedHistory
      .map((m) => {
        const role = m.role === 'user' ? 'ì‚¬ìš©ì' : m.role === 'assistant' ? 'ì–´ì‹œìŠ¤í„´íŠ¸' : 'ì‹œìŠ¤í…œ'
        return `- ${role}: ${String(m.content || '').trim()}`
      })
      .join('\n')
    const prompt = buildKoreanLegalPrompt(contextText, sanitizedQuery, historyText)

    const chatMessage = {
      role: 'user' as const,
      content: prompt,
    }

    if (wantsStream) {
      const controller = new AbortController()
      let aborted = false
      const onClose = () => {
        aborted = true
        controller.abort()
      }
      res.on('close', onClose)

      const stream = await openai.responses.create(
        {
          model: models.chat,
          input: prompt,
          stream: true,
        },
        { signal: controller.signal as any }
      )
      try {
        writePlainTextHeaders(res)
        writeCitations(res, usedSections, sanitizedQuery)

        // ë©”íƒ€ë°ì´í„° ì¶”ì 
        let responseId: string | undefined
        let modelId: string | undefined
        let usage: any | undefined

        for await (const event of stream as any) {
          const type = event?.type
          if (type === 'response.created') {
            responseId = event?.response?.id ?? event?.data?.id ?? responseId
            modelId = event?.response?.model ?? event?.data?.model ?? modelId
          } else if (type === 'response.output_text.delta') {
            const delta = event.delta || ''
            if (delta) await writeWithBackpressure(res, delta)
          } else if (type === 'response.completed') {
            // ì™„ë£Œ ì‹œì ì˜ ë©”íƒ€ ìˆ˜ì§‘ ì‹œë„
            const r = event?.response ?? event?.data
            responseId = r?.id ?? responseId
            modelId = r?.model ?? modelId
            usage = r?.usage ?? usage
            break
          } else if (type === 'response.error') {
            console.error('ğŸš¨ OpenAI streaming error event:', event)
          }
          if (aborted) break
        }
        res.write(`\n\n<!-- END_CITATIONS: ${usedSections.length} sources used -->`)
        res.end()
      } catch (streamErr) {
        console.error('ğŸš¨ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', streamErr)
        if (!res.headersSent) {
          res.status(500).json({ error: 'Streaming failed' })
        } else if (!res.writableEnded) {
          res.write(`\n\n<!-- STREAM_ERROR: Streaming failed -->`)
          res.end()
        }
      } finally {
        res.off('close', onClose)
      }
    } else {
      const completion = await openai.responses.create({
        model: models.chat,
        input: prompt,
      })

      const answer = (completion as any).output_text ?? ''
      sendTextWithCitations(res, answer, usedSections, sanitizedQuery)
    }
  } catch (err: unknown) {
    if (err instanceof UserError) {
      res.status(400).json({
        error: err.message,
        data: err.data,
      })
    } else if (err instanceof ApplicationError) {
      // í•œê¸€ ì£¼ì„: ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜ ì²˜ë¦¬
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    } else {
      console.error('ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì„œë²„ ì˜¤ë¥˜:', err)
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    }
  }
}
