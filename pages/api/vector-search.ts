import type { NextApiRequest, NextApiResponse } from 'next'
import { createClient } from '@supabase/supabase-js'
import { codeBlock, oneLine } from 'common-tags'
import GPT3Tokenizer from 'gpt3-tokenizer'
import {
  Configuration,
  OpenAIApi,
  CreateModerationResponse,
  CreateEmbeddingResponse,
  ChatCompletionRequestMessage,
} from 'openai-edge'
import { OpenAIStream, StreamingTextResponse } from 'ai'
import { ApplicationError, UserError } from '@/lib/errors'

const openAiKey = process.env.OPENAI_KEY
const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY

const config = new Configuration({
  apiKey: openAiKey,
})
const openai = new OpenAIApi(config)

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  try {
    console.log('ğŸš€ Vector search API í˜¸ì¶œ ì‹œì‘')
    console.log('ğŸ“‹ ìš”ì²­ ë©”ì„œë“œ:', req.method)
    console.log('ğŸ”§ í™˜ê²½ë³€ìˆ˜ í™•ì¸:', {
      hasOpenAiKey: !!openAiKey,
      hasSupabaseUrl: !!supabaseUrl,
      hasSupabaseServiceKey: !!supabaseServiceKey,
    })

    if (!openAiKey) {
      throw new ApplicationError('Missing environment variable OPENAI_KEY')
    }

    if (!supabaseUrl) {
      throw new ApplicationError('Missing environment variable SUPABASE_URL')
    }

    if (!supabaseServiceKey) {
      throw new ApplicationError('Missing environment variable SUPABASE_SERVICE_ROLE_KEY')
    }

    const requestData = req.body
    console.log('ğŸ“¦ ìš”ì²­ ë°ì´í„° íƒ€ì…:', typeof requestData)
    console.log('ğŸ“¦ ìš”ì²­ ë°ì´í„° ì¡´ì¬ì—¬ë¶€:', !!requestData)

    if (!requestData) {
      throw new UserError('Missing request data')
    }

    const { prompt: query } = requestData
    console.log('ğŸ” ì¿¼ë¦¬ ê¸¸ì´:', query?.length || 0)
    console.log('ğŸ” ì¿¼ë¦¬ ë¯¸ë¦¬ë³´ê¸°:', query?.substring(0, 100))

    if (!query) {
      throw new UserError('Missing query in request data')
    }

    const supabaseClient = createClient(supabaseUrl, supabaseServiceKey)

    // Moderate the content to comply with OpenAI T&C
    const sanitizedQuery = query.trim()
    console.log('ğŸ›¡ï¸ OpenAI ê²€ì—´ ì‹œì‘...')

    const moderationResponse: CreateModerationResponse = await openai
      .createModeration({ input: sanitizedQuery })
      .then((res) => res.json())

    console.log('ğŸ›¡ï¸ ê²€ì—´ ì‘ë‹µ:', moderationResponse)
    console.log('ğŸ›¡ï¸ ê²€ì—´ ì™„ë£Œ, ê²°ê³¼:', moderationResponse.results?.length > 0 ? 'OK' : 'ERROR')

    // Vercel í™˜ê²½ì—ì„œ moderationResponse.resultsê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (
      !moderationResponse.results ||
      !Array.isArray(moderationResponse.results) ||
      moderationResponse.results.length === 0
    ) {
      console.log('âŒ ê²€ì—´ ì‘ë‹µì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ:', moderationResponse)
      throw new ApplicationError('Invalid moderation response from OpenAI')
    }

    const [results] = moderationResponse.results

    if (results.flagged) {
      console.log('ğŸš« ì½˜í…ì¸  í”Œë˜ê·¸ë¨:', results.categories)
      throw new UserError('Flagged content', {
        flagged: true,
        categories: results.categories,
      })
    }

    // Create embedding from query
    console.log('ğŸ”¢ ì„ë² ë”© ìƒì„± ì‹œì‘...')
    const embeddingResponse = await openai.createEmbedding({
      model: 'text-embedding-3-small',
      input: sanitizedQuery.replaceAll('\n', ' '),
    })

    console.log('ğŸ”¢ ì„ë² ë”© ì‘ë‹µ ìƒíƒœ:', embeddingResponse.status)
    if (embeddingResponse.status !== 200) {
      throw new ApplicationError('Failed to create embedding for question', embeddingResponse)
    }

    const embeddingData: CreateEmbeddingResponse = await embeddingResponse.json()
    console.log('ğŸ”¢ ì„ë² ë”© ì‘ë‹µ êµ¬ì¡°:', {
      hasData: !!embeddingData.data,
      dataLength: embeddingData.data?.length,
    })

    // Vercel í™˜ê²½ì—ì„œ embedding dataê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (
      !embeddingData.data ||
      !Array.isArray(embeddingData.data) ||
      embeddingData.data.length === 0
    ) {
      console.log('âŒ ì„ë² ë”© ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ:', embeddingData)
      throw new ApplicationError('Invalid embedding response from OpenAI')
    }

    const [{ embedding }] = embeddingData.data

    console.log('ğŸ”¢ ì„ë² ë”© ìƒì„± ì™„ë£Œ, ì°¨ì›:', embedding?.length || 'unknown')

    console.log('ğŸ—„ï¸ Supabase RPC í˜¸ì¶œ ì‹œì‘...')
    const { error: matchError, data: pageSections } = await supabaseClient.rpc(
      'match_page_sections',
      {
        embedding,
        match_threshold: 0.78,
        match_count: 10,
        min_content_length: 50,
      }
    )

    console.log('ğŸ—„ï¸ RPC ì‘ë‹µ:', {
      hasError: !!matchError,
      errorMessage: matchError?.message,
      pageSectionsType: typeof pageSections,
      pageSectionsLength: Array.isArray(pageSections) ? pageSections.length : 'N/A',
      isArray: Array.isArray(pageSections),
    })

    if (matchError) {
      console.log('âŒ Supabase RPC ì˜¤ë¥˜:', matchError)
      throw new ApplicationError('Failed to match page sections', matchError)
    }

    // Vercel í™˜ê²½ì—ì„œ pageSectionsê°€ undefinedì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°©ì–´ì  ì²˜ë¦¬
    if (!pageSections || !Array.isArray(pageSections)) {
      console.log('âŒ ìœ íš¨í•˜ì§€ ì•Šì€ pageSections:', { pageSections, type: typeof pageSections })
      throw new ApplicationError('No matching page sections found')
    }

    console.log('ğŸ“ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œì‘...')
    const tokenizer = new GPT3Tokenizer({ type: 'gpt3' })
    let tokenCount = 0
    let contextText = ''

    for (let i = 0; i < pageSections.length; i++) {
      const pageSection = pageSections[i]
      console.log(`ğŸ“„ ì„¹ì…˜ ${i} ì²˜ë¦¬ ì¤‘:`, {
        hasSection: !!pageSection,
        hasContent: !!pageSection?.content,
        contentType: typeof pageSection?.content,
        contentLength: pageSection?.content?.length || 0,
      })

      // ì„¹ì…˜ ë°ì´í„° ë°©ì–´ì  ì²´í¬
      if (!pageSection || !pageSection.content) {
        console.log(`âš ï¸ ì„¹ì…˜ ${i} ìŠ¤í‚µ: ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°`)
        continue
      }

      const content = pageSection.content
      const encoded = tokenizer.encode(content)
      tokenCount += encoded.text.length

      if (tokenCount >= 1500) {
        console.log('âš ï¸ í† í° í•œë„ ë„ë‹¬, ì„¹ì…˜ ì²˜ë¦¬ ì¤‘ë‹¨:', { ì„¹ì…˜ìˆ˜: i, í† í°ìˆ˜: tokenCount })
        break
      }

      contextText += `${content.trim()}\n---\n`
    }

    console.log('ğŸ“ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ:', {
      ì´ì„¹ì…˜ìˆ˜: pageSections.length,
      ì²˜ë¦¬ëœì„¹ì…˜ìˆ˜: contextText.split('---').length - 1,
      ìµœì¢…í† í°ìˆ˜: tokenCount,
      ì»¨í…ìŠ¤íŠ¸ê¸¸ì´: contextText.length,
    })

    // ì•ˆì „í•œ í…œí”Œë¦¿ ìƒì„±ì„ ìœ„í•œ ë³€ìˆ˜ë“¤ í™•ì¸
    const safeContextText = contextText || ''
    const safeSanitizedQuery = sanitizedQuery || ''

    console.log('ğŸ“‹ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤€ë¹„:', {
      contextTextLength: safeContextText.length,
      queryLength: safeSanitizedQuery.length,
      hasCodeBlock: typeof codeBlock === 'function',
      hasOneLine: typeof oneLine === 'function',
    })

    const prompt = codeBlock`
      ${oneLine`
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë²•ì  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ 
        ì‹ ì¤‘í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ë§ˆí¬ë‹¤ìš´ 
        í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ì—ëŠ” "ì œê³µëœ ì •ë³´ë¡œëŠ” 
        ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤."ë¼ê³  
        ë‹µë³€í•´ì£¼ì„¸ìš”.
      `}

      ë²•ì  ì •ë³´:
      ${safeContextText}

      ì§ˆë¬¸: """
      ${safeSanitizedQuery}
      """

      ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
      1. ì •í™•í•˜ê³  ì‹ ì¤‘í•œ ë²•ì  ì¡°ì–¸ ì œê³µ
      2. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ íŒë¡€ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰
      3. êµ¬ì²´ì ì¸ ì‚¬ì•ˆì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ ë³€í˜¸ì‚¬ ìƒë‹´ ê¶Œìœ 
      4. ë©´ì±… ì¡°í•­ í¬í•¨ (ì¼ë°˜ì  ì •ë³´ ì œê³µ ëª©ì )
      
      ë‹µë³€:
    `

    console.log('ğŸ“‹ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ, ê¸¸ì´:', prompt?.length || 'unknown')

    const chatMessage: ChatCompletionRequestMessage = {
      role: 'user',
      content: prompt,
    }

    console.log('ğŸ¤– GPT ì™„ë£Œ ìš”ì²­ ì‹œì‘...')
    const response = await openai.createChatCompletion({
      model: 'gpt-4.1',
      messages: [chatMessage],
      max_tokens: 512,
      temperature: 0,
      stream: true,
    })

    console.log('ğŸ¤– GPT ì‘ë‹µ ìƒíƒœ:', response.status)
    if (!response.ok) {
      const error = await response.json()
      console.log('âŒ GPT ì™„ë£Œ ì‹¤íŒ¨:', error)
      throw new ApplicationError('Failed to generate completion', error)
    }

    console.log('ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘...')
    // Transform the response into a readable stream
    const stream = OpenAIStream(response)
    const reader = stream.getReader()
    let chunkCount = 0
    while (true) {
      const { done, value } = await reader.read()
      if (done) {
        console.log('âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ, ì´ ì²­í¬:', chunkCount)
        break
      }
      chunkCount++
      res.write(value)
    }
    res.end()
  } catch (err: unknown) {
    console.log('ğŸ’¥ API ì˜¤ë¥˜ ë°œìƒ:', err)
    if (err instanceof UserError) {
      console.log('ğŸ‘¤ ì‚¬ìš©ì ì˜¤ë¥˜:', err.message, err.data)
      res.status(400).json({
        error: err.message,
        data: err.data,
      })
    } else if (err instanceof ApplicationError) {
      // Print out application errors with their additional data
      console.error(`ğŸ”§ ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜: ${err.message}: ${JSON.stringify(err.data)}`)
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    } else {
      // Print out unexpected errors as is to help with debugging
      console.error('ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:', err)
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    }
  }
}
