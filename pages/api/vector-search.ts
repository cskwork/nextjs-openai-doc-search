import type { NextApiRequest, NextApiResponse } from 'next'
import { createClient } from '@supabase/supabase-js'
import { codeBlock, oneLine } from 'common-tags'
import { encode as encodeTokens } from 'gpt-tokenizer'
import OpenAI from 'openai'
import { ApplicationError, UserError } from '@/lib/errors'

// ê³µìš© íƒ€ì…
type UsedSection = {
  id: number
  path: string
  heading: string
  similarity: number
  content_length: number
  token_count: number
}

// ì‘ë‹µ í—¬í¼: ê³µí†µ í—¤ë” ì„¤ì •
function writePlainTextHeaders(res: NextApiResponse) {
  if (!res.headersSent) {
    res.writeHead(200, {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      Connection: 'keep-alive',
    })
  }
}

// ì‘ë‹µ í—¬í¼: ì¸ìš© ì£¼ì„ ì „ì†¡
function writeCitations(res: NextApiResponse, sources: UsedSection[], query: string) {
  const citationData = {
    type: 'citations',
    sources,
    query,
    timestamp: new Date().toISOString(),
  }
  res.write(`<!-- CITATIONS: ${JSON.stringify(citationData)} -->\n`)
}

// ì‘ë‹µ í—¬í¼: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì‘ë‹µ ì „ì†¡
function sendTextWithCitations(
  res: NextApiResponse,
  body: string,
  sources: UsedSection[],
  query: string
) {
  // í•œê¸€ ì£¼ì„: í…ìŠ¤íŠ¸ ë³¸ë¬¸ê³¼ ì¸ìš© ë©”íƒ€ë¥¼ í•¨ê»˜ ì „ì†¡
  writePlainTextHeaders(res)
  writeCitations(res, sources, query)
  res.write(body)
  res.write(`\n\n<!-- END_CITATIONS: ${sources.length} sources used -->`)
  res.end()
}

// ìœ í‹¸: ë¶ˆë¦¬ì–¸ íŒŒì‹± ("true"/true í—ˆìš©)
function parseBooleanFlag(value: unknown): boolean {
  return value === true || value === 'true'
}

// ìœ í‹¸: ì„œë²„ìš© Supabase í´ë¼ì´ì–¸íŠ¸
function createServerSupabaseClient(url: string, serviceKey: string) {
  return createClient(url, serviceKey, {
    auth: { persistSession: false, autoRefreshToken: false },
  })
}

const openAiKey = process.env.OPENAI_KEY
const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY

// ëª¨ë¸ êµ¬ì„±: í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
const CHAT_MODEL = process.env.OPENAI_CHAT_MODEL || 'gpt-5-mini'
const MODERATION_MODEL = process.env.OPENAI_MODERATION_MODEL || 'omni-moderation-latest'
const EMBEDDING_MODEL = process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small'

// @ts-ignore - OpenAI v4 SDK default export is a constructible client
const openai = new OpenAI({ apiKey: openAiKey })

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  try {
    // í•œê¸€ ì£¼ì„: ìš”ì²­ ë° í™˜ê²½ ì²´í¬
    console.log('ğŸš€ Vector search API í˜¸ì¶œ')
    if (req.method !== 'POST') {
      return res.status(405).json({ error: 'Method Not Allowed' })
    }

    if (!openAiKey) {
      throw new ApplicationError('Missing environment variable OPENAI_KEY')
    }

    if (!supabaseUrl) {
      throw new ApplicationError('Missing environment variable NEXT_PUBLIC_SUPABASE_URL')
    }

    if (!supabaseServiceKey) {
      throw new ApplicationError('Missing environment variable SUPABASE_SERVICE_ROLE_KEY')
    }

    const requestData = req.body
    if (!requestData) {
      throw new UserError('Missing request data')
    }

    const { prompt: query } = requestData
    const wantsStream = parseBooleanFlag(requestData?.stream)

    if (!query) {
      throw new UserError('Missing query in request data')
    }

    const supabaseClient = createServerSupabaseClient(supabaseUrl, supabaseServiceKey)

    // Moderate the content to comply with OpenAI T&C
    const sanitizedQuery = query.trim()
    const moderationResponse = await openai.moderations.create({
      model: MODERATION_MODEL,
      input: sanitizedQuery,
    })

    // Vercel í™˜ê²½ì—ì„œ moderationResponse.resultsê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (
      !moderationResponse.results ||
      !Array.isArray(moderationResponse.results) ||
      moderationResponse.results.length === 0
    ) {
      throw new ApplicationError('Invalid moderation response from OpenAI')
    }

    const [results] = moderationResponse.results

    if (results.flagged) {
      throw new UserError('Flagged content', {
        flagged: true,
        categories: results.categories,
      })
    }

    // LLM ê¸°ë°˜ ì¸í…íŠ¸ ë¶„ë¥˜
    let classifiedIntent: string = 'legal_question'
    let classifiedConfidence = 0
    const tryParseIntentJson = (text: string) => {
      try {
        return JSON.parse(text)
      } catch (_) {
        const m = text.match(/\{[\s\S]*\}/)
        if (m) {
          return JSON.parse(m[0])
        }
        return null
      }
    }
    try {
      const intentSystem = oneLine`
        ë‹¹ì‹ ì€ í•œêµ­ì–´ ë²•ë¥  ìƒë‹´ ë„ë©”ì¸ì˜ ì¸í…íŠ¸ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
        "greeting" | "legal_question" | "smalltalk" | "non_legal" | "other".
        ë°˜ë“œì‹œ ì—„ê²©í•œ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. í˜•ì‹: {"intent":"...","confidence":0.0~1.0}
        ì„¤ëª…, ì¶”ê°€ í…ìŠ¤íŠ¸, ì½”ë“œë¸”ë¡ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.`
      const intentResp = await openai.chat.completions.create({
        model: CHAT_MODEL,
        messages: [
          { role: 'system', content: intentSystem },
          { role: 'user', content: sanitizedQuery },
        ],
      })
      const rawIntentText = intentResp.choices?.[0]?.message?.content ?? ''
      console.log('ğŸ§­ ì¸í…íŠ¸ ì›ë¬¸ ì‘ë‹µ:', rawIntentText)
      const parsed = tryParseIntentJson(rawIntentText)
      if (parsed?.intent) classifiedIntent = String(parsed.intent)
      if (typeof parsed?.confidence === 'number') classifiedConfidence = parsed.confidence
      if (!parsed) {
        console.warn('âš ï¸ ì¸í…íŠ¸ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©')
      }
    } catch (e) {
      console.warn('âš ï¸ ì¸í…íŠ¸ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©')
    }

    // ì¸ì‚¬/ìŠ¤ëª°í†¡/ë¹„ë²•ë¥  ëŒ€ì‘ì€ RAG ìƒëµ í›„ ì¦‰ì‹œ ì‘ë‹µ
    if (classifiedIntent === 'greeting' || classifiedIntent === 'smalltalk') {
      const greetingAnswer = [
        'ì•ˆë…•í•˜ì„¸ìš”! ë²•ë¬´ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì–´ë–¤ ë²•ì  ë¬¸ì˜ë¥¼ ë„ì™€ë“œë¦´ê¹Œìš”?\n',
        '- ì˜ˆ: ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?\n',
        '- ì˜ˆ: ì§ì¥ì—ì„œ ë¶€ë‹¹í•œ ëŒ€ìš°ë¥¼ ë°›ì•˜ì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n',
        '- ì˜ˆ: ì„ëŒ€ì°¨ ê³„ì•½ ë§Œë£Œ í›„ ë³´ì¦ê¸ˆ ë°˜í™˜ ì ˆì°¨ê°€ ê¶ê¸ˆí•´ìš”.\n',
        '\ní•„ìš”í•˜ì‹œë‹¤ë©´ ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ë„ ë„ì™€ë“œë¦´ê²Œìš”. ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼)ê³¼ ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?'
      ].join('\n')
      sendTextWithCitations(res, greetingAnswer, [], sanitizedQuery)
      return
    }

    if (classifiedIntent === 'non_legal' || classifiedIntent === 'other') {
      // í•œê¸€ ì£¼ì„: ë¹„ë²•ë¥ /ê¸°íƒ€ ì£¼ì œì˜ ê²½ìš°ì—ë„ ê°„ë‹¨í•œ ì¼ë°˜ ì •ë³´ ì‘ë‹µì„ ì œê³µí•˜ê³  ìƒë‹´ ì—°ê²°ë¡œ ë¶€ë“œëŸ½ê²Œ ìœ ë„
      try {
        const nonLegalSystem = oneLine`
          ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” í•œêµ­ì–´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë²•ë¥  'ì™¸' ì£¼ì œì— ëŒ€í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¼ë°˜ ì •ë³´ ìˆ˜ì¤€ìœ¼ë¡œë§Œ ê°„ë‹¨íˆ(2~3ë¬¸ì¥) ë‹µí•©ë‹ˆë‹¤.
          ì „ë¬¸ì  ì¡°ì–¸ì´ë‚˜ í™•ì •ì  ë‹¨ì •ì€ í”¼í•˜ê³ , ì•ˆì „í•œ ë²”ìœ„ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”. ë§íˆ¬ëŠ” ì‚¬ìš©ì ì…ë ¥ì˜ í†¤ì„ ê°€ë³ê²Œ ë°˜ì˜í•˜ë˜ ê¸°ë³¸ì€ ì¡´ëŒ“ë§ì…ë‹ˆë‹¤.
          ì˜¤ì§ ê°„ê²°í•œ ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.`
        const nlResp = await openai.chat.completions.create({
          model: CHAT_MODEL,
          messages: [
            { role: 'system', content: nonLegalSystem },
            { role: 'user', content: sanitizedQuery },
          ],
        })
        const shortAnswer = nlResp.choices?.[0]?.message?.content?.trim() ?? ''
        const guidanceTail = [
          '',
          'í˜¹ì‹œ ë²•ì  ì´ìŠˆë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´, ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.',
          'ìƒë‹´ì„ ì›í•˜ì‹œë©´ ì„±í•¨ê³¼ ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼), ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?',
          'í˜„ì¬ ìƒí™©ì„ í•œë‘ ë¬¸ì¥ìœ¼ë¡œë§Œ ë§ë¶™ì—¬ ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ê²Œìš”.',
        ].join('\n')
        const finalAnswer = [shortAnswer, guidanceTail].join('\n')
        sendTextWithCitations(res, finalAnswer, [], sanitizedQuery)
      } catch (e) {
        // í•œê¸€ ì£¼ì„: ì‹¤íŒ¨ ì‹œì—ë„ ëŒ€í™”ë¥¼ ëŠì§€ ì•Šê³  ì•ˆë‚´ ë° CTA ì œê³µ
        const fallback = [
          'ê°„ë‹¨íˆ ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ê·¸ë˜ë„ ê±±ì • ë§ˆì„¸ìš”.',
          'ë²•ì  ì´ìŠˆë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.\n',
          'ìƒë‹´ì„ ì›í•˜ì‹œë©´ ì„±í•¨ê³¼ ì„ í˜¸í•˜ì‹œëŠ” ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼), ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?',
        ].join('\n')
        sendTextWithCitations(res, fallback, [], sanitizedQuery)
      }
      return
    }

    // Create embedding from query
    const embeddingResponse = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: sanitizedQuery.replaceAll('\n', ' '),
    })

    // Vercel í™˜ê²½ì—ì„œ embedding dataê°€ undefinedì¼ ìˆ˜ ìˆìŒ
    if (!embeddingResponse.data || !Array.isArray(embeddingResponse.data) || embeddingResponse.data.length === 0) {
      throw new ApplicationError('Invalid embedding response from OpenAI')
    }

    const [{ embedding }] = embeddingResponse.data

    // Supabase RPC í˜¸ì¶œ

    const { error: matchError, data: pageSections } = await supabaseClient.rpc(
      'match_page_sections',
      {
        embedding,
        match_threshold: 0.5, // ì„ê³„ê°’ì„ ë‚®ì¶¤ (0.78 â†’ 0.5)
        match_count: 10,
        min_content_length: 30, // ìµœì†Œ ê¸¸ì´ë„ ë‚®ì¶¤ (50 â†’ 30)
      }
    )

    if (matchError) {
      throw new ApplicationError('Failed to match page sections', matchError)
    }

    // Vercel í™˜ê²½ì—ì„œ pageSectionsê°€ undefinedì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°©ì–´ì  ì²˜ë¦¬
    if (!pageSections || !Array.isArray(pageSections)) {
      throw new ApplicationError('No matching page sections found')
    }

    let tokenCount = 0
    let contextText = ''
    const usedSections: UsedSection[] = []

    for (let i = 0; i < pageSections.length; i++) {
      const pageSection = pageSections[i]
      // ì„¹ì…˜ ë°ì´í„° ë°©ì–´ì  ì²´í¬
      if (!pageSection || !pageSection.content) {
        continue
      }

      const content = pageSection.content
      const sectionTokenCount = encodeTokens(content).length
      
      if (tokenCount + sectionTokenCount >= 1500) {
        break
      }

      tokenCount += sectionTokenCount
      contextText += `${content.trim()}\n---\n`
      
      // ì‚¬ìš©ëœ ì„¹ì…˜ ë©”íƒ€ë°ì´í„° ì €ì¥
      usedSections.push({
        id: pageSection.id || i,
        path: pageSection.path || 'unknown',
        heading: pageSection.heading || 'ì œëª© ì—†ìŒ',
        similarity: pageSection.similarity || 0,
        content_length: content.length,
        token_count: sectionTokenCount,
      })
    }

    // ì•ˆì „í•œ í…œí”Œë¦¿ ìƒì„±ì„ ìœ„í•œ ë³€ìˆ˜ë“¤ í™•ì¸
    const safeContextText = contextText || ''
    const safeSanitizedQuery = sanitizedQuery || ''

    const prompt = codeBlock`
      ${oneLine`
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  'ì •ë³´'ë¥¼ ì•ˆë‚´í•˜ëŠ” ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ 'ë²•ì  ì •ë³´' ë²”ìœ„ ë‚´ì—ì„œë§Œ ì‚¬ì‹¤ì— ê·¼ê±°í•´,
        ì‰¬ìš´ í•œêµ­ì–´ì™€ ì¡´ëŒ“ë§ë¡œ ë‹µí•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì •í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.
      `}

      ë‹µë³€ ì›ì¹™:
      - ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
      - ì–´ë ¤ìš´ ìš©ì–´ëŠ” ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ í’€ì–´ ì„¤ëª…
      - ì „ë¬¸ ë²•ë¥  ìë¬¸ì´ í•„ìš”í•œ ì§€ì ì€ ëª…í™•íˆ í‘œì‹œí•˜ê³ , ë³€í˜¸ì‚¬ ìƒë‹´ì„ ê¶Œìœ 
      - ë‹µë³€ ë§ˆì§€ë§‰ì— ì§§ì€ í›„ì† ì§ˆë¬¸ 1ê°œë¥¼ í¬í•¨í•´ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ê¸°
      - ì‚¬ìš©ìê°€ ì›í•  ê²½ìš° ë³€í˜¸ì‚¬ ìƒë‹´ ì—°ê²°ì„ ì •ì¤‘íˆ ì œì•ˆí•˜ê³ , ì„ í˜¸ ì—°ë½ ë°©ë²•(ì „í™”/ì´ë©”ì¼)ê³¼ ê°€ëŠ¥ ì‹œê°„ì„ ë¬¼ì–´ë³´ê¸°
      - ì‚¬ìš©ì ë§íˆ¬ë¥¼ ê°€ë³ê²Œ ë°˜ì˜í•˜ë˜, ê¸°ë³¸ì€ ì¡´ëŒ“ë§ë¡œ ê³µì†í•˜ê²Œ ì‘ë‹µí•˜ê¸°

      ë²•ì  ì •ë³´:
      ${safeContextText}

      ì§ˆë¬¸: """
      ${safeSanitizedQuery}
      """

      ë§Œì•½ ì œê³µëœ ë²•ì  ì •ë³´ë§Œìœ¼ë¡œ ì¶©ë¶„íˆ ë‹µí•˜ê¸° ì–´ë µë‹¤ë©´ ë‹¤ìŒì²˜ëŸ¼ ë§í•˜ì„¸ìš”:
      "ì œê³µëœ ì •ë³´ë¡œëŠ” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤."
    `

    const chatMessage = {
      role: 'user' as const,
      content: prompt,
    }

    if (wantsStream) {
      const responseStream = await openai.chat.completions.create({
        model: CHAT_MODEL,
        messages: [chatMessage],
        stream: true,
      })
      try {
        writePlainTextHeaders(res)
        writeCitations(res, usedSections, sanitizedQuery)

        // Stream chunks from the official OpenAI SDK
        let chunkCount = 0
        for await (const chunk of responseStream) {
          const delta = chunk.choices?.[0]?.delta?.content
          if (delta) {
            chunkCount++
            res.write(delta)
          }
        }
        res.write(`\n\n<!-- END_CITATIONS: ${usedSections.length} sources used -->`)
        res.end()
      } catch (streamErr) {
        console.error('ğŸš¨ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', streamErr)
        if (!res.headersSent) {
          res.status(500).json({ error: 'Streaming failed' })
        } else if (!res.writableEnded) {
          res.write(`\n\n<!-- STREAM_ERROR: Streaming failed -->`)
          res.end()
        }
      }
    } else {
      const completion = await openai.chat.completions.create({
        model: CHAT_MODEL,
        messages: [chatMessage],
      })

      const answer = completion.choices?.[0]?.message?.content ?? ''
      sendTextWithCitations(res, answer, usedSections, sanitizedQuery)
    }
  } catch (err: unknown) {
    if (err instanceof UserError) {
      res.status(400).json({
        error: err.message,
        data: err.data,
      })
    } else if (err instanceof ApplicationError) {
      // í•œê¸€ ì£¼ì„: ì• í”Œë¦¬ì¼€ì´ì…˜ ì˜¤ë¥˜ ì²˜ë¦¬
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    } else {
      console.error('ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì„œë²„ ì˜¤ë¥˜:', err)
      res.status(500).json({
        error: 'There was an error processing your request',
      })
    }
  }
}
